module Scorers
  class EmpiricalScorer
    HEDGED_PATTERNS = [/as an ai language model/i, /cannot assist with/i, /i cannot/i].freeze

    def self.evaluate(prompt, runs: 2)
      text = prompt.to_s
      Rails.logger.info("[EmpiricalScorer] START runs=#{runs} text_len=#{text.length}") if defined?(Rails)
      t0 = Process.clock_gettime(Process::CLOCK_MONOTONIC) rescue Time.now.to_f
      # Run calls in parallel to cap total latency
      threads = []
      outputs = Array.new(runs)
      runs.times do |i|
        threads << Thread.new do
          ti = Process.clock_gettime(Process::CLOCK_MONOTONIC) rescue Time.now.to_f
          begin
            out = Llm::OpenaiClient.run_prompt(text, temperature: 0.2, timeout: 30, max_retries: 1, max_tokens: 16384)
            outputs[i] = out
          ensure
            ei = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) rescue Time.now.to_f) - ti) * 1000.0
            Rails.logger.info("[EmpiricalScorer] run##{i+1} output_len=#{outputs[i].to_s.length} elapsed=#{ei.round}ms") if defined?(Rails)
          end
        end
      end
      threads.each(&:join)
      Rails.logger.info("[EmpiricalScorer] outputs sizes=#{outputs.map { |o| o.to_s.length }}") if defined?(Rails)

      reasons = []
      details = {}
      score = 0

      # Use AI to evaluate the outputs (0..100 total)
      ai_score, ai_reasons = ai_evaluate_outputs(text, outputs)
      Rails.logger.info("[EmpiricalScorer] ai_score=#{ai_score} ai_reasons=#{ai_reasons}") if defined?(Rails)
      score += ai_score
      reasons.concat(ai_reasons) if ai_reasons.any?

      score = [[score.round, 0].max, 100].min
      total = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) rescue Time.now.to_f) - t0) * 1000.0
      Rails.logger.info("[EmpiricalScorer] final_score=#{score} total_elapsed=#{total.round}ms") if defined?(Rails)
      { score: score, reasons: reasons, details: details }
    rescue => e
      { score: 0, reasons: ["empirical error: #{e.message}"], details: { variance: 1.0 } }
    end

    def self.ai_evaluate_outputs(prompt, outputs)
      return 0, [] if outputs.empty? || outputs.all?(&:blank?)

      # Combine outputs for analysis
      output1 = outputs[0].to_s.strip
      output2 = outputs[1].to_s.strip

      return 0, ["No valid outputs to evaluate"] if output1.empty? && output2.empty?

      # Create evaluation prompt for AI judge
      evaluation_prompt = <<~PROMPT
        You are an expert prompt evaluator. I need you to evaluate two outputs generated by the same prompt and provide a score based on:

        1. QUALITY (0-60 points): How good, useful, and comprehensive are the outputs?
           - Consider: completeness, accuracy, usefulness, level of detail, technical correctness. Overall, does the output fulfill the request / answer the question of the prompt?

        2. CONSISTENCY (0-10 points): How similar and consistent are the two outputs?
           - Consider: core logic, key information, structure, approach

        3. FORMAT (0-30 points): How well-structured and appropriate are the outputs?
           - Consider: organization, clarity, adherence to any requested format, readability

        Original prompt: "#{prompt}"

        Output 1:
        #{output1}

        Output 2:
        #{output2}

        Provide your evaluation as JSON with this exact structure:
        {
          "quality_score": number (0-40),
          "consistency_score": number (0-40),
          "format_score": number (0-20),
          "total_score": number (0-100),
          "reasons": [
            "Brief explanation of quality assessment",
            "Brief explanation of consistency assessment",
            "Brief explanation of format assessment"
          ]
        }

        Be fair and objective. Focus on the outputs' merit, not the original prompt quality.
        Only return valid JSON, no other text.
      PROMPT

      begin
        # Use LLM to evaluate the outputs
        response = Llm::OpenaiClient.run_prompt(
          evaluation_prompt,
          temperature: 0.1,  # Low temperature for consistent evaluation
          timeout: 15,       # Shorter timeout for evaluation call
          max_tokens: 1000   # Reasonable limit for evaluation response
        )

        # Parse JSON response
        parsed = JSON.parse(response)
        score = parsed['total_score'].to_i.clamp(0, 100)
        reasons = parsed['reasons'] || []

        Rails.logger.info("[EmpiricalScorer] AI evaluation complete - score=#{score}") if defined?(Rails)
        [score, reasons]

      rescue => e
        Rails.logger.error("[EmpiricalScorer] AI evaluation failed: #{e.message}") if defined?(Rails)
        # Fallback to manual scoring if AI evaluation fails
        format_points, format_msgs, _ = format_score(prompt, outputs)
        variance, consistency_points = consistency_score(outputs)
        quality_points = quality_score(outputs)

        fallback_score = format_points + consistency_points + quality_points
        fallback_reasons = []
        fallback_reasons.concat(format_msgs.map { |m| "Output structure: #{m}." }) if format_msgs.any?
        fallback_reasons << "The model produced similar outputs across runs, indicating a stable prompt." if consistency_points >= 30
        fallback_reasons << "The response appears substantive and actionable." if quality_points >= 15

        [fallback_score.clamp(0, 100), fallback_reasons]
      end
    end
    private_class_method :ai_evaluate_outputs

    def self.format_score(prompt, outputs)
      points = 0
      messages = []
      
      # Instead of checking if JSON/list was requested, check if outputs are well-structured
      outputs.each do |out|
        next unless out
        output_text = out.to_s.strip
        next if output_text.empty?
        
        structure_points = 0
        
        # 1. Check for structured content (any format)
        has_structure = false
        
        # JSON structure
        if output_text.match?(/\{[\s\S]*\}/) && output_text.match?(/"[\w_]+"\s*:/)
          structure_points += 10
          messages << 'Structured JSON-like output detected' unless messages.include?('Structured JSON-like output detected')
          has_structure = true
        end
        
        # Code structure (functions, classes, methods)
        if output_text.match?(/\b(def|function|class|const|let|var)\s+\w+/i)
          structure_points += 10
          messages << 'Code structure detected' unless messages.include?('Code structure detected')
          has_structure = true
        end
        
        # List structure (bullets, numbered, etc.)
        if output_text.match?(/(?:^|\n)\s*[-*â€¢]\s+\w+/) || output_text.match?(/(?:^|\n)\s*\d+\.\s+\w+/)
          structure_points += 8
          messages << 'List structure detected' unless messages.include?('List structure detected')
          has_structure = true
        end
        
        # Table/columnar structure
        if output_text.match?(/\|.*\|/) && output_text.scan(/\|/).count >= 4
          structure_points += 8
          messages << 'Table structure detected' unless messages.include?('Table structure detected')
          has_structure = true
        end
        
        # 2. Check for organization (paragraphs, sections, etc.)
        if output_text.lines.count >= 3
          structure_points += 5
        end
        
        # 3. If no clear structure, check if it's a coherent response
        if !has_structure && output_text.length >= 50
          structure_points += 5
          messages << 'Coherent text response' unless messages.include?('Coherent text response')
        end
        
        points += [structure_points, 20].min
      end
      
      ctx = { has_structure: points > 0 }
      [[points, 40].min, messages, ctx]
    end
    private_class_method :format_score

    # Add this new method before format_score
    def self.detect_json_request(prompt)
      text = prompt.downcase
      # Direct JSON mentions
      return true if text.include?('json')
      
      # Format requests
      return true if text.match?(/\b(format|return|output|provide)\s+(as\s+)?json\b/)
      return true if text.match?(/\bjson\s+(format|object|response|output)\b/)
      
      # Structure requests that imply JSON
      return true if text.include?('keys') && text.include?('{')
      return true if text.match?(/\bkeys?\s*[:=]\s*\[/)
      
      false
    end
    private_class_method :detect_json_request

    def self.extract_json_from_text(text)
      return nil if text.to_s.strip.empty?
      s = text.dup
      # Strip ```json fences if present
      if s =~ /```json([\s\S]*?)```/i
        return $1.strip
      end
      # Fallback: grab the first {...} balanced region
      start = s.index('{')
      return nil unless start
      # naive scan to last '}'
      last = s.rindex('}')
      return nil unless last && last > start
      candidate = s[start..last]
      candidate
    end
    private_class_method :extract_json_from_text

    def self.consistency_score(outputs)
      return [1.0, 0] if outputs.length < 2
      a, b = outputs[0].to_s, outputs[1].to_s
      return [0.0, 40] if a == b && a.length > 0
      
      distance = normalized_edit_distance(a, b)
      variance = distance
      
      similarity = 1.0 - distance
      
      adjusted_similarity = similarity ** 0.3
      
      points = (adjusted_similarity * 40.0)
      
      [variance, points.round.clamp(0, 40)]
    end

    def self.quality_score(outputs)
      return 0 if outputs.empty?
      
      total_points = 0
      valid_outputs = 0
      
      outputs.each do |out|
        output_text = out.to_s
        next if output_text.strip.empty?
        next if HEDGED_PATTERNS.any? { |r| output_text.match?(r) }
        
        pts = 0
        
        # Length-based scoring
        if output_text.length >= 200
          pts += 10
        elsif output_text.length >= 100
          pts += 8
        elsif output_text.length >= 50
          pts += 5
        elsif output_text.length >= 20
          pts += 3
        else
          pts += 1
        end
        
        pts += 3 if output_text =~ /\d/
        pts += 2 if output_text.match?(/\b(step|process|method|way|how|what|why|when|where)\b/i)
        pts += 2 if output_text.match?(/\b(first|second|third|next|then|finally|also|additionally)\b/i)
        pts += 3 if output_text.match?(/\b(def|function|class)\s+\w+/)  # Function/class definitions
        pts += 2 if output_text.match?(/^[\s]*#.*$/m)  # Comments (Ruby/Python)
        pts += 2 if output_text.match?(/\/\/|\/\*/)  # Comments (JS/C-style)
        pts += 2 if output_text.match?(/\b(rescue|except|catch|raise|throw|error)\b/i)  # Error handling
        pts += 2 if output_text.match?(/\b(if|unless|case|when|switch)\b/)  # Conditional logic
        pts += 1 if output_text.match?(/\b(require|import|include|using)\b/)  # Library usage
        
        total_points += pts
        valid_outputs += 1
      end
      
      return 0 if valid_outputs == 0
      (total_points.to_f / valid_outputs).round.clamp(0, 20)
    end

    def self.normalized_edit_distance(a, b)
      return 0.0 if a == b
      return 1.0 if a.empty? || b.empty?
      dist = levenshtein(a, b)
      dist.to_f / [a.length, b.length].max
    end
    private_class_method :normalized_edit_distance

    def self.levenshtein(a, b)
      m = a.length
      n = b.length
      d = Array.new(m + 1) { Array.new(n + 1) }
      (0..m).each { |i| d[i][0] = i }
      (0..n).each { |j| d[0][j] = j }
      (1..m).each do |i|
        (1..n).each do |j|
          cost = a[i - 1] == b[j - 1] ? 0 : 1
          d[i][j] = [
            d[i - 1][j] + 1,
            d[i][j - 1] + 1,
            d[i - 1][j - 1] + cost
          ].min
        end
      end
      d[m][n]
    end
    private_class_method :levenshtein
  end
end


